{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "copyrighted-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "# import sentencepiece as spm\n",
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "turned-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scene(scene):\n",
    "    scene = scene.replace(' ','')\n",
    "    map_scene = {}\n",
    "    edge_0, edge_1 = re.split(r'\\),|\\)，',scene)\n",
    "    ent0, ent1 = edge_0.split('(')\n",
    "    map_scene[ent0] = ent1\n",
    "    \n",
    "    ent2, ent3 = edge_1[:-1].split('(')\n",
    "    map_scene[ent2] = ent3\n",
    "    return map_scene\n",
    "def process_var(var):\n",
    "    map_var = {}\n",
    "    edge_0, edge_1 = re.split(r'\\),|\\)，',var)\n",
    "    ent0, ent1 = edge_0.split('(')\n",
    "    map_var[ent1] = ent0.replace(' ','')\n",
    "    ent2, ent3 = edge_1[:-1].split('(')\n",
    "    map_var[ent3] = ent2.replace(' ','')\n",
    "    return map_var\n",
    "def process_mr(mr, var):\n",
    "    splited_mr = mr.split(',')\n",
    "    mr_dict = {}\n",
    "    for one in splited_mr:\n",
    "        ent0, ent1 = one.split('[')\n",
    "        mr_dict[ent0] = ent1[:-1]\n",
    "    processed_var = process_var(var)\n",
    "    mr_dict['x_ent'] = processed_var['x']\n",
    "    mr_dict['y_ent'] = processed_var['y']\n",
    "    return mr_dict\n",
    "def fenge_B006(text):\n",
    "    a = re.split(r'(\\+|,|-|\\*|/|=|!=|>=|<=|>|<|\\(|\\)|\\[|\\]|\\\\|\\|\\b\\d+\\w*\\b|[\\u4E00-\\u9FA5]|\\②|\\①|\\③|\\④|\\㉖|\\:)', text)\n",
    "    return [x.strip() for x in a if x!='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "capable-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_excel('../../我处理了一下的数据/train_equ.xlsx', index_col=None)\n",
    "dev_set = pd.read_excel('../../我处理了一下的数据/dev_equ.xlsx')\n",
    "test_set = pd.read_excel('../../我处理了一下的数据/test_equ.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heard-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_equation_mr(equation1, equation2):\n",
    "    equ1_left = equation1[:equation1.find('=')]\n",
    "    equ1_right = equation1[equation1.find('=')+1:]\n",
    "    splited_left_1 = fenge_B006(equ1_left)\n",
    "    splited_right_1 = fenge_B006(equ1_right)\n",
    "    one_mr = []\n",
    "    # 先处理第一个式子的左半部分\n",
    "    if len(splited_left_1) == 3:\n",
    "        # 处理x部分\n",
    "        if splited_left_1[0] =='x':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq1_x_index[{}]'.format(splited_left_1[0].replace('x','')))\n",
    "        # 处理中间的那个运算符\n",
    "        one_mr.append('eq1_left_sym2[{}]'.format(splited_left_1[1]))\n",
    "        # 处理y部分\n",
    "        if splited_left_1[2] =='y':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq1_y_index[{}]'.format(splited_left_1[2].replace('y','')))\n",
    "    elif len(splited_left_1) == 4:\n",
    "        # 有四个部分，看起来都要做填充\n",
    "        # 第一个部分的符号\n",
    "        one_mr.append('eq1_left_sym1[{}]'.format(splited_left_1[0]))\n",
    "        if splited_left_1[1] =='x':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq1_x_index[{}]'.format(splited_left_1[1].replace('x','')))\n",
    "        # 第二个部分的符号\n",
    "        one_mr.append('eq1_left_sym2[{}]'.format(splited_left_1[2]))\n",
    "        # 处理y部分\n",
    "        if splited_left_1[3] =='y':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq1_y_index[{}]'.format(splited_left_1[3].replace('y','')))\n",
    "    else:\n",
    "        raise 'wrong input equation1, check preprocessing'\n",
    "    \n",
    "    # 处理第一个式子的右半部分\n",
    "    if len(splited_right_1) == 1:\n",
    "        # 只有一个实数，应该大部分都是如此\n",
    "        one_mr.append('eq1_right_num1[{}]'.format(splited_right_1[0]))\n",
    "    elif len(splited_right_1) == 3:\n",
    "        # 有实数的计算存在，好吧，那就算一算吧\n",
    "        one_mr.append('eq1_right_num1[{}]'.format(splited_right_1[0]))\n",
    "        one_mr.append('eq1_right_sym[{}]'.format(splited_right_1[1]))\n",
    "        one_mr.append('eq1_right_num2[{}]'.format(splited_right_1[2]))\n",
    "    \n",
    "    \n",
    "    equ2_left = equation2[:equation2.find('=')]\n",
    "    equ2_right = equation2[equation2.find('=')+1:]\n",
    "    splited_left_2 = fenge_B006(equ2_left)\n",
    "    splited_right_2 = fenge_B006(equ2_right)\n",
    "    \n",
    "    # 先处理第二个式子的左半部分\n",
    "    if len(splited_left_2) == 3:\n",
    "        # 处理x部分\n",
    "        if splited_left_2[0] =='x':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq2_x_index[{}]'.format(splited_left_2[0].replace('x','')))\n",
    "        # 处理中间的那个运算符\n",
    "        one_mr.append('eq2_left_sym2[{}]'.format(splited_left_2[1]))\n",
    "        # 处理y部分\n",
    "        if splited_left_2[2] =='y':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq2_y_index[{}]'.format(splited_left_2[2].replace('y','')))\n",
    "    elif len(splited_left_2) == 4:\n",
    "        # 有四个部分，看起来都要做填充\n",
    "        # 第一个部分的符号\n",
    "        one_mr.append('eq2_left_sym1[{}]'.format(splited_left_2[0]))\n",
    "        if splited_left_2[1] =='x':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq2_x_index[{}]'.format(splited_left_2[1].replace('x','')))\n",
    "        # 第二个部分的符号\n",
    "        one_mr.append('eq2_left_sym2[{}]'.format(splited_left_2[2]))\n",
    "        # 处理y部分\n",
    "        if splited_left_2[3] =='y':\n",
    "            pass\n",
    "        else:\n",
    "            one_mr.append('eq2_y_index[{}]'.format(splited_left_2[3].replace('y','')))\n",
    "    else:\n",
    "        raise 'wrong input equation2, check preprocessing'\n",
    "    \n",
    "    # 处理第一个式子的右半部分\n",
    "    if len(splited_right_2) == 1:\n",
    "        # 只有一个实数，应该大部分都是如此\n",
    "        one_mr.append('eq2_right_num1[{}]'.format(splited_right_2[0]))\n",
    "    elif len(splited_right_2) == 3:\n",
    "        # 有实数的计算存在，好吧，那就算一算吧\n",
    "        one_mr.append('eq2_right_num1[{}]'.format(splited_right_2[0]))\n",
    "        one_mr.append('eq2_right_sym[{}]'.format(splited_right_2[1]))\n",
    "        one_mr.append('eq2_right_num2[{}]'.format(splited_right_2[2]))\n",
    "    return ','.join(one_mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tracked-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mr = []\n",
    "for idx, row in train_set.iterrows():\n",
    "    equ1, equ2 = row['关系1_trans'], row['关系2_trans']\n",
    "    new_mr.append(get_equation_mr(equ1, equ2))\n",
    "train_set['mr'] = new_mr\n",
    "new_mr = []\n",
    "for idx, row in dev_set.iterrows():\n",
    "    equ1, equ2 = row['关系1_trans'], row['关系2_trans']\n",
    "    new_mr.append(get_equation_mr(equ1, equ2))\n",
    "dev_set['mr'] = new_mr\n",
    "new_mr = []\n",
    "for idx, row in test_set.iterrows():\n",
    "    equ1, equ2 = row['关系1_trans'], row['关系2_trans']\n",
    "    new_mr.append(get_equation_mr(equ1, equ2))\n",
    "test_set['mr'] = new_mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "complimentary-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biparty(head, rel, tail,eqn='eq1'):\n",
    "    one_dummy = 'dummy_cal'\n",
    "    one_edge, one_node = [],[one_dummy]\n",
    "    if rel == '+':\n",
    "        one_edge += [(head, '和关系_{}'.format(eqn)), ('和关系_{}'.format(eqn), one_dummy), \n",
    "                     (one_dummy, '和关系_{}_rev'.format(eqn)), ('和关系_{}_rev'.format(eqn), head),\n",
    "                     (tail, '和关系_{}'.format(eqn)), ('和关系_{}_rev'.format(eqn), tail)]\n",
    "        one_node += [head, tail, '和关系_{}'.format(eqn),'和关系_{}_rev'.format(eqn)]\n",
    "    if rel == '-':\n",
    "        one_edge += [(head,'被减数关系_{}'.format(eqn)), ('被减数关系_{}'.format(eqn), one_dummy),\n",
    "                     (one_dummy,'被减数关系_{}_rev'.format(eqn)), ('被减数关系_{}_rev'.format(eqn), head),\n",
    "                     (tail,'减数关系_{}'.format(eqn)), ('减数关系_{}'.format(eqn),one_dummy),\n",
    "                    (one_dummy,'减数关系_{}_rev'.format(eqn)), ('减数关系_{}_rev'.format(eqn),tail)\n",
    "                    ]\n",
    "        one_node += [head, tail, '减数关系_{}'.format(eqn), '被减数关系_{}'.format(eqn),'减数关系_{}_rev'.format(eqn), '被减数关系_{}_rev'.format(eqn)]\n",
    "    if rel == '*':\n",
    "        one_edge += [(head,'被乘数关系_{}'.format(eqn)), ('被乘数关系_{}'.format(eqn), one_dummy),\n",
    "                     (one_dummy,'被乘数关系_{}_rev'.format(eqn)), ('被乘数关系_{}_rev'.format(eqn), head),\n",
    "                     (tail,'乘数关系_{}'.format(eqn)), ('乘数关系_{}'.format(eqn),one_dummy),\n",
    "                    (one_dummy,'乘数关系_{}_rev'.format(eqn)), ('乘数关系_{}_rev'.format(eqn),tail)\n",
    "                    ]\n",
    "        one_node += [head, tail, '乘数关系_{}'.format(eqn), '被乘数关系_{}'.format(eqn), '乘数关系_{}_rev'.format(eqn), '被乘数关系_{}_rev'.format(eqn)]\n",
    "    if rel == '/':\n",
    "        one_edge += [(head,'被除数关系_{}'.format(eqn)), ('被除数关系_{}'.format(eqn), one_dummy),\n",
    "                     (one_dummy,'被除数关系_{}_rev'.format(eqn)), ('被除数关系_{}_rev'.format(eqn), head),\n",
    "                     (tail,'除数关系_{}'.format(eqn)), ('除数关系_{}'.format(eqn),one_dummy),\n",
    "                    (one_dummy,'除数关系_{}_rev'.format(eqn)), ('除数关系_{}_rev'.format(eqn),tail)\n",
    "                    ]\n",
    "        one_node += [head, tail, '除数关系_{}'.format(eqn), '被除数关系_{}'.format(eqn),'被除数关系_{}_rev'.format(eqn),'除数关系_{}_rev'.format(eqn)]\n",
    "\n",
    "    return one_edge, one_node, one_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "individual-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bild_dual_new_graph(equ_1, equ_2, scene, var, mr_dict, tou_info, jiao_info):\n",
    "    equ_1, equ_2 = equ_1.replace(' ',''), equ_2.replace(' ','')\n",
    "    \n",
    "    processed_var, processed_scene = process_var(var), process_scene(scene)\n",
    "    x_ent, y_ent = mr_dict['x_ent'], mr_dict['y_ent']\n",
    "    \n",
    "    # equation graph\n",
    "    all_node, all_edge = [x_ent, y_ent], []\n",
    "    \n",
    "    # common sense graph\n",
    "    all_node_1, all_edge_1 = [x_ent, y_ent], []\n",
    "    \n",
    "    #加入场景信息\n",
    "    for tail in re.split(r',|，',processed_scene[x_ent]):\n",
    "        all_edge_1 += [(x_ent,'belong_to_x'),('belong_to_x',tail), (tail,'belong_to_x_rev'), ('belong_to_x_rev',x_ent)]\n",
    "        all_node_1 += [tail,'belong_to_x','belong_to_x_rev']\n",
    "        \n",
    "    for tail in re.split(r',|，',processed_scene[y_ent]):\n",
    "        all_edge_1 += [(y_ent,'belong_to_y'),('belong_to_y',tail), (tail,'belong_to_y_rev'), ('belong_to_y_rev',y_ent)]\n",
    "        all_node_1 += [tail,'belong_to_y','belong_to_y_rev']\n",
    "    \n",
    "    # 处理头_entity, 头_unit, 脚_entity, 脚_unit\n",
    "    if tou_info['entity'] != '':\n",
    "        all_node_1 += [tou_info['entity'], '有头_ent','有头_ent_rev']\n",
    "        all_edge_1 += [(x_ent,'有头_ent'), ('有头_ent', tou_info['entity']), (tou_info['entity'],'有头_ent_rev'),('有头_ent_rev', x_ent),\n",
    "                     (y_ent, '有头_ent'), ('有头_ent_rev', y_ent)]\n",
    "    if jiao_info['entity'] != '':\n",
    "        all_node_1 += [jiao_info['entity'],'有脚_ent','有脚_ent_rev']\n",
    "        all_edge_1 += [(x_ent, '有脚_ent'), ('有脚_ent', jiao_info['entity']), (jiao_info['entity'],'有脚_ent_rev'),('有脚_ent_rev', x_ent),\n",
    "                     (y_ent, '有脚_ent'), ('有脚_ent_rev', y_ent)]\n",
    "    if tou_info['unit'] != '':\n",
    "        all_node_1 += [tou_info['unit'],'有头_unit','有头_unit_rev']\n",
    "        all_edge_1 += [(x_ent,'有头_unit'),('有头_unit', tou_info['unit']), (tou_info['unit'],'有头_unit_rev'),('有头_unit_rev', x_ent),\n",
    "                     (y_ent, '有头_unit'), ('有头_unit_rev', y_ent)]\n",
    "    if jiao_info['unit'] != '':\n",
    "        all_node_1 += [jiao_info['unit'],'有脚_unit','有脚_unit_rev']\n",
    "        all_edge_1 += [(x_ent,'有脚_unit'),('有脚_unit', jiao_info['unit']), (jiao_info['unit'],'有脚_unit_rev'),('有脚_unit_rev', x_ent),\n",
    "                     (y_ent, '有脚_unit'),('有脚_unit_rev', y_ent)]\n",
    "    \n",
    "    # 处理关系1左半部分\n",
    "    tmp_x_eq1, tmp_y_eq1, tmp_x_eq2, tmp_y_eq2 = x_ent, y_ent, x_ent, y_ent\n",
    "    tmp_right_eq1, tmp_right_eq2 = mr_dict['eq1_right_num1'], mr_dict['eq2_right_num1']\n",
    "    if 'eq1_x_index' in mr_dict:\n",
    "        all_edge += [(x_ent, '乘关系x'), ('乘关系x',mr_dict['eq1_x_index']), (mr_dict['eq1_x_index'],'乘关系x_rev'), ('乘关系x_rev',x_ent)]\n",
    "        all_node += ['乘关系x', mr_dict['eq1_x_index'],'乘关系x_rev']\n",
    "        tmp_x_eq1 = mr_dict['eq1_x_index']\n",
    "        \n",
    "    if 'eq1_y_index' in mr_dict:\n",
    "        all_edge += [(y_ent,'乘关系y'), ('乘关系y', mr_dict['eq1_y_index']), (mr_dict['eq1_y_index'],'乘关系y_rev'), ('乘关系y_rev',y_ent)]\n",
    "        all_node += ['乘关系y',mr_dict['eq1_y_index'], '乘关系y_rev']\n",
    "        tmp_y_eq1 = mr_dict['eq1_y_index']\n",
    "    \n",
    "    # 处理关系2左半部分\n",
    "    if 'eq2_x_index' in mr_dict:\n",
    "        all_edge += [(x_ent, '乘关系x'), ('乘关系x',mr_dict['eq2_x_index']), (mr_dict['eq2_x_index'],'乘关系x_rev'), ('乘关系x_rev',x_ent)]\n",
    "        all_node += ['乘关系x', mr_dict['eq2_x_index'], '乘关系x_rev']\n",
    "        tmp_x_eq2 = mr_dict['eq2_x_index']\n",
    "    \n",
    "    if 'eq2_y_index' in mr_dict:\n",
    "        all_edge += [(y_ent,'乘关系y'), ('乘关系y', mr_dict['eq2_y_index']), (mr_dict['eq2_y_index'],'乘关系y_rev'), ('乘关系y_rev',y_ent)]\n",
    "        all_node += ['乘关系y',mr_dict['eq2_y_index'], '乘关系y_rev']\n",
    "        tmp_y_eq2 = mr_dict['eq2_y_index']\n",
    "    \n",
    "    # 分析一下右边\n",
    "    if 'eq1_right_sym' in mr_dict:\n",
    "        # 右边存在一坨计算，需要算算\n",
    "        one_tmp_edge1, one_tmp_node1, one_tmp_right1 = get_biparty(mr_dict['eq1_right_num1'], mr_dict['eq1_right_sym'],mr_dict['eq1_right_num2'],'eq1')\n",
    "    else:\n",
    "        one_tmp_edge1, one_tmp_node1, one_tmp_right1 = [], [mr_dict['eq1_right_num1']], mr_dict['eq1_right_num1']\n",
    "            \n",
    "    if 'eq2_right_sym' in mr_dict:\n",
    "        one_tmp_edge2, one_tmp_node2, one_tmp_right2 = get_biparty(mr_dict['eq2_right_num1'], mr_dict['eq2_right_sym'],mr_dict['eq2_right_num2'],'eq2')\n",
    "    else:\n",
    "        one_tmp_edge2, one_tmp_node2, one_tmp_right2 = [], [mr_dict['eq2_right_num1']], mr_dict['eq2_right_num1']\n",
    "    \n",
    "    all_edge += one_tmp_edge1\n",
    "    all_edge += one_tmp_edge2\n",
    "    all_node += one_tmp_node1\n",
    "    all_node += one_tmp_node2\n",
    "    \n",
    "    # 结合左边和右边\n",
    "    if 'eq1_left_sym1' in mr_dict:\n",
    "        # 这个时候只可能是eq1sym1=负数，eq1sym2=正数\n",
    "        one_tmp_edge_inner_1, one_tmp_node_inner_1 = get_triparty(tmp_y_eq1, tmp_x_eq1, '-', one_tmp_right1,'eq1')\n",
    "    else:\n",
    "        one_tmp_edge_inner_1, one_tmp_node_inner_1 = get_triparty(tmp_x_eq1, tmp_y_eq1, mr_dict['eq1_left_sym2'], one_tmp_right1,'eq1')\n",
    "    \n",
    "    if 'eq2_left_sym1' in mr_dict:\n",
    "        one_tmp_edge_inner_2, one_tmp_node_inner_2 = get_triparty(tmp_y_eq2, tmp_x_eq2, '-', one_tmp_right2,'eq2')\n",
    "    else:\n",
    "        one_tmp_edge_inner_2, one_tmp_node_inner_2 = get_triparty(tmp_x_eq2, tmp_y_eq2, mr_dict['eq2_left_sym2'], one_tmp_right2,'eq2')\n",
    "    \n",
    "    \n",
    "    all_edge += one_tmp_edge_inner_1\n",
    "    all_edge += one_tmp_edge_inner_2\n",
    "    all_node += one_tmp_node_inner_1\n",
    "    all_node += one_tmp_node_inner_2\n",
    "    \n",
    "    \n",
    "    all_ele = [x for y in all_edge for x in y ]\n",
    "    assert set(all_ele)==set(all_node), 'wrong preprocessing code'\n",
    "    \n",
    "    return list(all_edge), list(set(all_node)), list(all_edge_1), list(set(all_node_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stone-suicide",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triparty(head_left_1, head_left_2, rel, right_one, eqn='eq1'):\n",
    "    one_edge, one_node = [],[]\n",
    "    if rel == '+':\n",
    "        one_edge += [(head_left_1, '和关系_{}_res'.format(eqn)), ('和关系_{}_res'.format(eqn), right_one), \n",
    "                     (right_one, '和关系_{}_rev_res'.format(eqn)), ('和关系_{}_rev_res'.format(eqn),head_left_1),\n",
    "                     (head_left_2, '和关系_{}_res'.format(eqn)), ('和关系_{}_rev_res'.format(eqn),head_left_2),]\n",
    "        one_node += [head_left_1, head_left_2, right_one, '和关系_{}_res'.format(eqn), '和关系_{}_rev_res'.format(eqn)]\n",
    "    if rel == '-':\n",
    "        one_edge += [(head_left_1,'被减数关系_{}_res'.format(eqn)), ('被减数关系_{}_res'.format(eqn), right_one),\n",
    "                     (right_one,'被减数关系_{}_rev_res'.format(eqn)), ('被减数关系_{}_rev_res'.format(eqn), head_left_1),\n",
    "                     (head_left_2,'减数关系_{}_res'.format(eqn)), ('减数关系_{}_res'.format(eqn),right_one),\n",
    "                     (right_one,'减数关系_{}_rev_res'.format(eqn)), ('减数关系_{}_rev_res'.format(eqn),head_left_2)\n",
    "                    ]\n",
    "        one_node += [head_left_1, head_left_2, '减数关系_{}_res'.format(eqn), '被减数关系_{}_res'.format(eqn), right_one, '减数关系_{}_rev_res'.format(eqn), '被减数关系_{}_rev_res'.format(eqn)]\n",
    "    if rel == '*':\n",
    "        one_edge += [(head_left_1,'被乘数关系_{}_res'.format(eqn)), ('被乘数关系_{}_res'.format(eqn), right_one),\n",
    "                     (right_one,'被乘数关系_{}_rev_res'.format(eqn)), ('被乘数关系_{}_rev_res'.format(eqn), head_left_1),\n",
    "                     (head_left_2,'乘数关系_{}_res'.format(eqn)), ('乘数关系_{}_res'.format(eqn),right_one),\n",
    "                    (right_one,'乘数关系_{}_rev_res'.format(eqn)), ('乘数关系_{}_rev_res'.format(eqn),head_left_2)\n",
    "                    ]\n",
    "        one_node += [head_left_1, head_left_2, '乘数关系_{}_res'.format(eqn), '被乘数关系_{}_res'.format(eqn),right_one, \n",
    "                     '乘数关系_{}_rev_res'.format(eqn), '被乘数关系_{}_rev_res'.format(eqn)]\n",
    "    if rel == '/':\n",
    "        one_edge += [(head_left_1,'被除数关系_{}_res'.format(eqn)), ('被除数关系_{}_res'.format(eqn), right_one),\n",
    "                     (right_one,'被除数关系_{}_rev_res'.format(eqn)), ('被除数关系_{}_rev_res'.format(eqn), head_left_1),\n",
    "                     (head_left_2,'除数关系_{}_res'.format(eqn)), ('除数关系_{}_res'.format(eqn),right_one),\n",
    "                    (right_one,'除数关系_{}_rev_res'.format(eqn)), ('除数关系_{}_rev_res'.format(eqn),head_left_2)\n",
    "                    ]\n",
    "        one_node += [head_left_1, head_left_2, '除数关系_{}_res'.format(eqn), '被除数关系_{}_res'.format(eqn),right_one,'被除数关系_{}_rev_res'.format(eqn), '除数关系_{}_rev_res'.format(eqn)]\n",
    "    return one_edge, one_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "processed-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:  \n",
    "    def __init__(self):\n",
    "        self.BOS_WORD = '<s>'\n",
    "        self.EOS_WORD = '</s>'\n",
    "        self.PAD_WORD = '<blank>'\n",
    "        self.UNK_WORD = '<unk>'\n",
    "        self.eq1_x_index_WORD = 'eq_one_x_index'\n",
    "        self.eq2_x_index_WORD = 'eq_two_x_index'\n",
    "        self.eq1_y_index_WORD = 'eq_one_y_index'\n",
    "        self.eq2_y_index_WORD = 'eq_two_y_index'\n",
    "        self.eq1_right_num1_WORD = 'eq_one_right_num_one'\n",
    "        self.eq2_right_num1_WORD = 'eq_two_right_num_one'\n",
    "        self.eq1_right_num2_WORD = 'eq_one_right_num_two'\n",
    "        self.eq2_right_num2_WORD = 'eq_two_right_num_two'\n",
    "        self.x_entity_WORD = 'x_entity'\n",
    "        self.y_entity_WORD = 'y_entity'\n",
    "        self.head_info_unit_WORD = 'head_info_unit'\n",
    "        self.jiao_info_unit_WORD = 'jiao_info_unit'\n",
    "        self.jiao_info_entity_WORD = 'jiao_info_entity'\n",
    "        self.head_info_entity_WORD = 'head_info_entity'\n",
    "#         self.dummy_equal_WORD = 'dummy_cal'\n",
    "        \n",
    "        self.PAD = 0\n",
    "        self.UNK = 1\n",
    "        self.BOS = 2\n",
    "        self.EOS = 3\n",
    "        self.eq1_x_index = 4\n",
    "        self.eq2_x_index = 5\n",
    "        self.eq1_y_index = 6\n",
    "        self.eq2_y_index = 7\n",
    "        self.eq1_right_num1 = 8\n",
    "        self.eq2_right_num1 = 9\n",
    "        self.eq1_right_num2 = 10\n",
    "        self.eq2_right_num2 = 11\n",
    "        self.x_entity = 12\n",
    "        self.y_entity = 13\n",
    "        self.head_info_unit = 14\n",
    "        self.jiao_info_unit = 15\n",
    "        self.jiao_info_entity =16\n",
    "        self.head_info_entity =17\n",
    "#         self.dummy_equal = 18\n",
    "Constants = Constants()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "laden-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "MR_FIELDS =['方程二右边数字一', '方程一右边数字一', 'x_entity','y_entity', 'head信息_entity',\n",
    "    '脚信息_entity', 'head信息_unit', '脚信息_unit', \n",
    "    '方程一y系数', '方程二y系数', '方程一x系数', '方程二x系数','方程一右边数字二','方程二右边数字二']\n",
    "\n",
    "MR_KEYMAP = dict((key, idx) for idx, key in enumerate(MR_FIELDS))\n",
    "MR_KEY_NUM = len(MR_FIELDS)\n",
    "\n",
    "lex_fields = ['方程二右边数字一', '方程一右边数字一', 'x_entity', 'y_entity', 'head信息_entity', '脚信息_entity',\n",
    "              'head信息_unit','脚信息_unit',\n",
    "    '方程一y系数', '方程二y系数', '方程一x系数', '方程二x系数', '方程一右边数字二','方程二右边数字二']\n",
    "\n",
    "lex_tar = [Constants.eq2_right_num1_WORD, Constants.eq1_right_num1_WORD, Constants.x_entity_WORD, Constants.y_entity_WORD, Constants.head_info_entity_WORD, Constants.jiao_info_entity_WORD,\n",
    "    Constants.head_info_unit_WORD, Constants.jiao_info_unit_WORD,\n",
    "    Constants.eq1_y_index_WORD,Constants.eq2_y_index_WORD,\n",
    "    Constants.eq1_x_index_WORD, Constants.eq2_x_index_WORD, Constants.eq1_right_num2_WORD, Constants.eq2_right_num2_WORD]\n",
    "\n",
    "lex_keymap = dict((key, idx) for idx, key in enumerate(lex_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "chemical-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jitu_mr_type(mr, scene, equ1, equ2, var, tou_info, jiao_info):\n",
    "    mr, scene, equ1, equ2, var = copy.deepcopy(mr), copy.deepcopy(scene), copy.deepcopy(equ1), copy.deepcopy(equ2), copy.deepcopy(var)\n",
    "    tou_info,jiao_info = eval(copy.deepcopy(tou_info)), eval(copy.deepcopy(jiao_info))\n",
    "    lex_this = [None] * len(lex_tar)\n",
    "    \n",
    "    mr_dict = process_mr(mr, var)\n",
    "#     print(mr_dict)\n",
    "    ent_x, ent_y = mr_dict['x_ent'], mr_dict['y_ent']\n",
    "    \n",
    "    # 替换实体的名称作为输入\n",
    "    if ent_y == ('超级'+ent_x):\n",
    "        scene = scene.replace(ent_y, Constants.y_entity_WORD)\n",
    "        mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "        lex_this[lex_keymap['y_entity']] = ent_y\n",
    "        scene = scene.replace(ent_x,Constants.x_entity_WORD)\n",
    "        mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "        lex_this[lex_keymap['x_entity']] = ent_x\n",
    "    if ent_x == ('另'+ent_y):\n",
    "        scene = scene.replace(ent_x, Constants.x_entity_WORD)\n",
    "        mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "        lex_this[lex_keymap['x_entity']] = ent_x\n",
    "        scene = scene.replace(ent_y,Constants.y_entity_WORD)\n",
    "        mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "        lex_this[lex_keymap['y_entity']] = ent_y\n",
    "    elif ent_y == ('另'+ent_x):\n",
    "        scene = scene.replace(ent_y,Constants.y_entity_WORD)\n",
    "        mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "        lex_this[lex_keymap['y_entity']] = ent_y\n",
    "        scene = scene.replace(ent_x, Constants.x_entity_WORD)\n",
    "        mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "        lex_this[lex_keymap['x_entity']] = ent_x\n",
    "    elif ('另一' in ent_x) or ('另外一' in ent_y):\n",
    "        scene = scene.replace(ent_y,Constants.y_entity_WORD)\n",
    "        mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "        lex_this[lex_keymap['y_entity']] = ent_y\n",
    "        scene = scene.replace(ent_x, Constants.x_entity_WORD)\n",
    "        mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "        lex_this[lex_keymap['x_entity']] = ent_x\n",
    "    elif ('另一' in ent_y) or ('另外一' in ent_x):\n",
    "        scene = scene.replace(ent_x, Constants.x_entity_WORD)\n",
    "        mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "        lex_this[lex_keymap['x_entity']] = ent_x\n",
    "        scene = scene.replace(ent_y,Constants.y_entity_WORD)\n",
    "        mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "        lex_this[lex_keymap['y_entity']] = ent_y\n",
    "    else:\n",
    "        if 'dummy' not in ent_x:\n",
    "            scene = scene.replace(ent_x,Constants.x_entity_WORD)\n",
    "            mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "            lex_this[lex_keymap['x_entity']] = ent_x\n",
    "            scene = scene.replace(ent_y,Constants.y_entity_WORD)\n",
    "            mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "            lex_this[lex_keymap['y_entity']] = ent_y\n",
    "\n",
    "    items = mr.split(',')\n",
    "    for idx, item in enumerate(items):\n",
    "        key, raw_val = item.split('[')\n",
    "#         if (key=='x_ent') & ('dummy' not in raw_val):\n",
    "#             scene = scene.replace(ent_x,Constants.x_entity_WORD)\n",
    "#             mr_dict['x_ent'] = Constants.x_entity_WORD\n",
    "#             lex_this[lex_keymap['x_entity']] = ent_x\n",
    "#         elif (key =='y_ent') & ('dummy' not in raw_val):\n",
    "#             scene = scene.replace(ent_y,Constants.y_entity_WORD)\n",
    "#             mr_dict['y_ent'] = Constants.y_entity_WORD\n",
    "#             lex_this[lex_keymap['y_entity']] = ent_y\n",
    "        if key=='eq1_x_index':\n",
    "            mr_dict['eq1_x_index'] = Constants.eq1_x_index_WORD\n",
    "            lex_this[lex_keymap['方程一x系数']] = raw_val[:-1]\n",
    "        elif key == 'eq1_y_index':\n",
    "            mr_dict['eq1_y_index'] = Constants.eq1_y_index_WORD\n",
    "            lex_this[lex_keymap['方程一y系数']] = raw_val[:-1]\n",
    "        elif key == 'eq1_left_sym2':\n",
    "            pass\n",
    "        elif key == 'eq1_right_num1':\n",
    "            if mr_dict['eq1_right_num1'] == '0':\n",
    "                mr_dict['eq1_right_num1'] = '0'\n",
    "                lex_this[lex_keymap['方程一右边数字一']] = '0'\n",
    "            else:\n",
    "                mr_dict['eq1_right_num1'] = Constants.eq1_right_num1_WORD\n",
    "                lex_this[lex_keymap['方程一右边数字一']] = raw_val[:-1]\n",
    "        elif key == 'eq1_right_num2':\n",
    "            mr_dict['eq1_right_num2'] = Constants.eq1_right_num2_WORD\n",
    "            lex_this[lex_keymap['方程一右边数字二']] = raw_val[:-1]\n",
    "        elif key == 'eq2_x_index':\n",
    "            mr_dict['eq2_x_index'] = Constants.eq2_x_index_WORD\n",
    "            lex_this[lex_keymap['方程二x系数']] = raw_val[:-1]\n",
    "        elif key == 'eq2_y_index':\n",
    "            mr_dict['eq2_y_index'] = Constants.eq2_y_index_WORD\n",
    "            lex_this[lex_keymap['方程二y系数']] = raw_val[:-1]\n",
    "        elif key == 'eq2_left_sym2':\n",
    "            pass\n",
    "        elif key == 'eq2_right_num1':\n",
    "            if mr_dict['eq2_right_num1'] == '0':\n",
    "                mr_dict['eq2_right_num1'] = '0'\n",
    "                lex_this[lex_keymap['方程一右边数字一']] = '0'\n",
    "            else:\n",
    "                mr_dict['eq2_right_num1'] = Constants.eq2_right_num1_WORD\n",
    "                lex_this[lex_keymap['方程二右边数字一']] = raw_val[:-1]\n",
    "        elif key == 'eq2_right_num2':\n",
    "            mr_dict['eq2_right_num2'] = Constants.eq2_right_num2_WORD\n",
    "            lex_this[lex_keymap['方程二右边数字二']] = raw_val[:-1]\n",
    "        else:\n",
    "            pass\n",
    "    # 一些特殊case，需要做一下处理，这里面是为了应对除法的情况\n",
    "    if lex_this[lex_keymap['方程二右边数字一']] == lex_this[lex_keymap['方程一右边数字一']]:\n",
    "        mr_dict['eq2_right_num1'] = Constants.eq1_right_num1_WORD\n",
    "    # 特殊case 如果鸡兔互换\n",
    "    if (lex_this[lex_keymap['方程一x系数']] == lex_this[lex_keymap['方程二y系数']]) & (lex_this[lex_keymap['方程二x系数']] == lex_this[lex_keymap['方程一y系数']]):\n",
    "        mr_dict['eq2_y_index'] = Constants.eq1_x_index_WORD\n",
    "        mr_dict['eq2_x_index'] = Constants.eq1_y_index_WORD\n",
    "        \n",
    "    if tou_info['entity'] != '':\n",
    "        lex_this[lex_keymap['head信息_entity']] = tou_info['entity']\n",
    "        tou_info['entity'] = Constants.head_info_entity_WORD\n",
    "    if tou_info['unit'] != '':\n",
    "        lex_this[lex_keymap['head信息_unit']] = tou_info['unit']\n",
    "        tou_info['unit'] = Constants.head_info_unit_WORD\n",
    "    if jiao_info['entity'] != '':\n",
    "        lex_this[lex_keymap['脚信息_entity']] = jiao_info['entity']\n",
    "        jiao_info['entity'] = Constants.jiao_info_entity_WORD\n",
    "    if jiao_info['unit'] != '':\n",
    "        lex_this[lex_keymap['脚信息_unit']] = jiao_info['unit']\n",
    "        jiao_info['unit'] = Constants.jiao_info_unit_WORD\n",
    "    all_edge, all_node, all_edge_1, all_node_1 = bild_dual_new_graph(equ1, equ2, scene, var, mr_dict, tou_info, jiao_info)\n",
    "    return all_edge, all_node, lex_this, all_edge_1, all_node_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "revolutionary-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, nodes, lexs, edges_2, nodes_2 = [],[],[],[],[]\n",
    "for idx, row in train_set.iterrows():\n",
    "    equ_1, equ_2, scene, var, mr, tou_info, jiao_info = row['关系1_trans'], row['关系2_trans'], row['scene'], row['变量'],row['mr'], row['头信息'],row['脚信息']\n",
    "    edge_1, node_1, lex_1, edge_2, node_2 = process_jitu_mr_type(mr, scene, equ_1, equ_2, var, tou_info, jiao_info)\n",
    "    edges.append(edge_1)\n",
    "    nodes.append(node_1)\n",
    "    lexs.append(lex_1)\n",
    "    edges_2.append(edge_2)\n",
    "    nodes_2.append(node_2)\n",
    "train_set['edges'], train_set['nodes'], train_set['lexs'], train_set['edges_1'], train_set['nodes_1'] = edges, nodes, lexs, edges_2, nodes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sixth-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, nodes, lexs, edges_2, nodes_2 = [],[],[],[],[]\n",
    "for idx, row in dev_set.iterrows():\n",
    "    equ_1, equ_2, scene, var, mr, tou_info, jiao_info = row['关系1_trans'], row['关系2_trans'], row['scene'], row['变量'],row['mr'], row['头信息'],row['脚信息']\n",
    "    edge_1, node_1, lex_1, edge_2, node_2 = process_jitu_mr_type(mr, scene, equ_1, equ_2, var, tou_info, jiao_info)\n",
    "    edges.append(edge_1)\n",
    "    nodes.append(node_1)\n",
    "    lexs.append(lex_1)\n",
    "    edges_2.append(edge_2)\n",
    "    nodes_2.append(node_2)\n",
    "dev_set['edges'], dev_set['nodes'], dev_set['lexs'], dev_set['edges_1'], dev_set['nodes_1'] = edges, nodes, lexs, edges_2, nodes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "green-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, nodes, lexs, edges_2, nodes_2 = [],[],[],[],[]\n",
    "for idx, row in test_set.iterrows():\n",
    "    equ_1, equ_2, scene, var, mr, tou_info, jiao_info = row['关系1_trans'], row['关系2_trans'], row['scene'], row['变量'],row['mr'], row['头信息'],row['脚信息']\n",
    "    edge_1, node_1, lex_1, edge_2, node_2 = process_jitu_mr_type(mr, scene, equ_1, equ_2, var, tou_info, jiao_info)\n",
    "    edges.append(edge_1)\n",
    "    nodes.append(node_1)\n",
    "    lexs.append(lex_1)\n",
    "    edges_2.append(edge_2)\n",
    "    nodes_2.append(node_2)\n",
    "test_set['edges'], test_set['nodes'], test_set['lexs'], test_set['edges_1'], test_set['nodes_1'] = edges, nodes, lexs, edges_2, nodes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-thong",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sunset-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_after_text(text, lex_list):\n",
    "    if lex_list:\n",
    "        for word, tar in zip(lex_list, lex_tar):\n",
    "            if word is not None:\n",
    "                if word in text:\n",
    "                    text = text.replace(word, tar)\n",
    "    return text\n",
    "def tokenize_word(text, sp, lex_list=None):\n",
    "    words = []\n",
    "    if lex_list:\n",
    "        for word, tar in zip(lex_list, lex_tar):\n",
    "            if word is not None:\n",
    "                if word in text:\n",
    "                    text = text.replace(word, tar)\n",
    "    for frag in sp.EncodeAsPieces(text):\n",
    "        words.append(frag)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "composed-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取所有的场景信息\n",
    "scene_new = train_set['scene'].tolist()\n",
    "some_scene = set()\n",
    "for one_scene in scene_new:\n",
    "    one_scene = one_scene.replace(' ','')\n",
    "    real_scene = re.findall(r'[(](.*?)[)]',one_scene)\n",
    "    for sce in real_scene:\n",
    "        cc_sce = re.split(',|，', sce)\n",
    "        for cc in cc_sce:\n",
    "            some_scene.add(cc)\n",
    "all_scene = list(some_scene)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hawaiian-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_relation = [\n",
    "    'belong_to_x','belong_to_y','belong_to_x_rev','belong_to_y_rev',\n",
    "    '有头_ent','有头_ent_rev','有脚_ent','有脚_ent_rev',\n",
    "    '有头_unit','有头_unit_rev','有脚_unit', '有脚_unit_rev','乘关系x','乘关系x_rev','乘关系y','乘关系y_rev',\n",
    "    '和关系_eq1','和关系_eq2','被减数关系_eq1','被减数关系_eq2','减数关系_eq1','减数关系_eq2',\n",
    "    '被乘数关系_eq1','被乘数关系_eq2','乘数关系_eq1','乘数关系_eq2','被除数关系_eq1','被除数关系_eq2',\n",
    "    \"除数关系_eq1\",\"除数关系_eq2\"\n",
    "    '和关系_eq1_rev','和关系_eq2_rev','被减数关系_eq1_rev','被减数关系_eq2_rev','减数关系_eq1_rev','减数关系_eq2_rev',\n",
    "    '被乘数关系_eq1_rev','被乘数关系_eq2_rev','乘数关系_eq1_rev','乘数关系_eq2_rev','被除数关系_eq1_rev','被除数关系_eq2_rev',\n",
    "    \"除数关系_eq1_rev\",\"除数关系_eq2_rev\",\n",
    "    '减数关系_eq1_res','减数关系_eq1_rev_res','减数关系_eq2_res','减数关系_eq2_rev_res','和关系_eq1_res','和关系_eq1_rev_res',\n",
    "    '和关系_eq2_res','和关系_eq2_rev_res','被减数关系_eq1_res','被减数关系_eq1_rev_res','被减数关系_eq2_res','被减数关系_eq2_rev_res'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "running-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "miniature-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_sent = []\n",
    "for idx, row in train_set.iterrows():\n",
    "    text = row['ref']\n",
    "    one_edge, one_node, lex_one = row['edges'], row['nodes'], row['lexs']\n",
    "    after_sent.append(get_after_text(text, lex_one))\n",
    "with open('../processed_data/after_text_with_rev_res_little.txt','w') as f:\n",
    "    for line in after_sent:\n",
    "        f.write(line+ '\\n')\n",
    "text_path = '../processed_data/after_text_with_rev_res_little.txt'\n",
    "model_save_path = '../processed_data/encoded_with_rev_res_little'\n",
    "user_symbols = ','.join(lex_tar+all_scene+all_relation+['dummy1', 'dummy2'])\n",
    "spm.SentencePieceTrainer.Train('--input={} \\\n",
    "                            --model_prefix={} --model_type=bpe \\\n",
    "                            --user_defined_symbols={} \\\n",
    "                            --character_coverage=0.9996 --hard_vocab_limit=false'.format(\n",
    "                                text_path, model_save_path, user_symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "peaceful-savings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('../processed_data/encoded_with_rev_res_little.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "alien-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_idx(word_insts, min_word_count):\n",
    "    '''Trim vocab by number of occurence'''\n",
    "    full_vocab = set(w for sent in word_insts for w in sent)\n",
    "    print('[Info] Original Vocabulary size =', len(full_vocab))\n",
    "    word2idx = {\n",
    "        Constants.BOS_WORD: Constants.BOS,\n",
    "        Constants.EOS_WORD: Constants.EOS,\n",
    "        Constants.PAD_WORD: Constants.PAD,\n",
    "        Constants.UNK_WORD: Constants.UNK,\n",
    "        Constants.eq1_x_index_WORD: Constants.eq1_x_index,\n",
    "        Constants.eq2_x_index_WORD: Constants.eq2_x_index,\n",
    "        Constants.eq1_y_index_WORD: Constants.eq1_y_index,\n",
    "        Constants.eq2_y_index_WORD: Constants.eq2_y_index,\n",
    "        Constants.eq1_right_num1_WORD: Constants.eq1_right_num1,\n",
    "        Constants.eq2_right_num1_WORD: Constants.eq2_right_num1,\n",
    "        Constants.eq1_right_num2_WORD: Constants.eq1_right_num2,\n",
    "        Constants.eq2_right_num2_WORD: Constants.eq1_right_num2,\n",
    "        Constants.x_entity_WORD: Constants.x_entity,\n",
    "        Constants.y_entity_WORD: Constants.y_entity,\n",
    "        Constants.head_info_unit_WORD: Constants.head_info_unit,\n",
    "        Constants.jiao_info_unit_WORD: Constants.jiao_info_unit,\n",
    "        Constants.jiao_info_entity_WORD: Constants.jiao_info_entity,\n",
    "        Constants.head_info_entity_WORD: Constants.head_info_entity\n",
    "    }\n",
    "    \n",
    "    word_count = {w: 0 for w in full_vocab}\n",
    "    for sent in word_insts:\n",
    "        for word in sent:\n",
    "            word_count[word] += 1\n",
    "    ignored_word_count = 0\n",
    "    for word, count in word_count.items():\n",
    "        if word not in word2idx:\n",
    "            if count > min_word_count:\n",
    "                word2idx[word] = len(word2idx)\n",
    "            else:\n",
    "                ignored_word_count+=1\n",
    "    print('[Info] Trimmed vocabulary size = {},'.format(len(word2idx)), 'each with minimum occurence = {}'.format(min_word_count))\n",
    "    print('[Info] Ingored word count = {}'.format(ignored_word_count))\n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "academic-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scene(one_scene):\n",
    "    one_scene = one_scene.replace(' ','')\n",
    "    processed = process_scene(one_scene)\n",
    "    all_word = set()\n",
    "    for key, value in processed.items():\n",
    "        for one_word in re.split(',|，', value):\n",
    "            all_word.add(one_word)\n",
    "    return list(all_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "equal-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_some_ref, train_some_scene,train_lex = [],[],[]\n",
    "for idx, row in train_set.iterrows():\n",
    "    edge, scene, node, ref, lex = row['edges'], row['scene'], row['nodes'], row['ref'], row['lexs']\n",
    "    tokenized_sent = tokenize_word(ref, sp, lex)\n",
    "    train_some_ref.append(tokenized_sent)\n",
    "    train_some_scene.append(get_scene(scene))\n",
    "    train_lex.append(lex)\n",
    "dev_some_ref, dev_some_scene,dev_lex = [],[],[]\n",
    "for idx, row in dev_set.iterrows():\n",
    "    edge, scene, node, ref, lex = row['edges'], row['scene'], row['nodes'], row['ref'], row['lexs']\n",
    "    tokenized_sent = tokenize_word(ref, sp, lex)\n",
    "    dev_some_ref.append(tokenized_sent)\n",
    "    dev_some_scene.append(get_scene(scene))\n",
    "    dev_lex.append(lex)\n",
    "test_some_ref, test_some_scene,test_lex = [],[],[]\n",
    "for idx, row in test_set.iterrows():\n",
    "    edge, scene, node, ref, lex = row['edges'], row['scene'], row['nodes'], row['ref'], row['lexs']\n",
    "    tokenized_sent = tokenize_word(ref, sp, lex)\n",
    "    test_some_ref.append(tokenized_sent)\n",
    "    test_some_scene.append(get_scene(scene))\n",
    "    test_lex.append(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "secondary-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes_0= train_set['nodes'].tolist() #equation info node\n",
    "train_nodes_1 = train_set['nodes_1'].tolist() #common sense info node\n",
    "train_edges_0 = train_set['edges'].tolist() #equation info edge\n",
    "train_edges_1 = train_set['edges_1'].tolist() #common sense info edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aquatic-membership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Original Vocabulary size = 1821\n",
      "[Info] Trimmed vocabulary size = 1825, each with minimum occurence = 0\n",
      "[Info] Ingored word count = 0\n"
     ]
    }
   ],
   "source": [
    "word2idx = build_vocab_idx(train_nodes_0 + train_nodes_1 + train_some_ref, min_word_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sought-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'dict':{\n",
    "        'tgt': word2idx\n",
    "    },\n",
    "    \"train\":{\n",
    "        'scene': train_some_scene,\n",
    "        'ref': train_some_ref,\n",
    "        'lexs': train_lex\n",
    "    },\n",
    "    \"dev\":{\n",
    "        'scene': dev_some_scene,\n",
    "        \"ref\": dev_some_ref,\n",
    "        'lexs': dev_lex,\n",
    "    },\n",
    "    \"test\":{\n",
    "        'scene': test_some_scene,\n",
    "        \"ref\": test_some_ref,\n",
    "        'lexs': test_lex\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "solid-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save一下dual graph的数据\n",
    "data['train']['node_1'] = train_set['nodes'].tolist() #equation info node\n",
    "data['train']['node_2'] = train_set['nodes_1'].tolist() #common sense info node\n",
    "data['train']['edge_1'] = train_set['edges'].tolist() #equation info edge\n",
    "data['train']['edge_2'] = train_set['edges_1'].tolist() #common sense info edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "following-journalism",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dev']['node_1'] = dev_set['nodes'].tolist() #equation info node\n",
    "data['dev']['node_2'] = dev_set['nodes_1'].tolist() #common sense info node\n",
    "data['dev']['edge_1'] = dev_set['edges'].tolist() #equation info edge\n",
    "data['dev']['edge_2'] = dev_set['edges_1'].tolist() #common sense info edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "timely-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['test']['node_1'] = test_set['nodes'].tolist() #equation info node\n",
    "data['test']['node_2'] = test_set['nodes_1'].tolist() #common sense info node\n",
    "data['test']['edge_1'] = test_set['edges'] .tolist()#equation info edge\n",
    "data['test']['edge_2'] = test_set['edges_1'].tolist() #common sense info edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "urban-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data, '../processed_data/dual_graph_rev_res_little.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-coverage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-recipient",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-northern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-sharp",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-chorus",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tq_env_cpm",
   "language": "python",
   "name": "tq_env_cpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
